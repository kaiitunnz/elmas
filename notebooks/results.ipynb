{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict, List, NamedTuple, Optional, Tuple, Union\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from dotenv import load_dotenv\n",
    "from matplotlib.axes import Axes\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "mpl.rcParams[\"figure.dpi\"] = 150\n",
    "\n",
    "ROOT_DIR = Path.cwd().parent\n",
    "ROOT_RESULT_DIR = ROOT_DIR / \"results\"\n",
    "\n",
    "INDEX_COLS = [\"method\", \"iteration\"]\n",
    "METRIC_COLS = [\n",
    "    \"duration\",\n",
    "    \"output_throughput\",\n",
    "    \"request_throughput\",\n",
    "    \"median_ttft_ms\",\n",
    "    \"mean_ttft_ms\",\n",
    "    \"gpu_prefix_cache_hit_rate\",\n",
    "    \"cpu_prefix_cache_hit_rate\",\n",
    "]\n",
    "\n",
    "METHOD_NAMES = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(NamedTuple):\n",
    "    col: str\n",
    "    name: str\n",
    "    ylim: Optional[Tuple[float, float]] = None\n",
    "    plot_errors: bool = True\n",
    "\n",
    "\n",
    "def get_method_names(result_dir: Path) -> List[str]:\n",
    "    default_method_names = [\n",
    "        \"NO-APC\",\n",
    "        \"APC\",\n",
    "        \"MT-APC\",\n",
    "        \"MT-APC-ONLY-MT\",\n",
    "        \"MT-APC-ONLY-ASYNC\",\n",
    "        \"MT-APC-ONLY-SCHED\",\n",
    "        \"MT-APC-NO-PREFETCH\",\n",
    "        \"MT-APC-NO-SCHED\",\n",
    "    ]\n",
    "\n",
    "    result_files = [\n",
    "        file\n",
    "        for file in (result_dir / \"cleaned\").iterdir()\n",
    "        if file.is_file() and file.suffix == \".csv\"\n",
    "    ]\n",
    "    df = pd.read_csv(result_files[0])\n",
    "    method_names = df[\"method\"].drop_duplicates().apply(lambda x: x.upper()).tolist()\n",
    "    method_names = [name for name in default_method_names if name in method_names]\n",
    "    return method_names\n",
    "\n",
    "\n",
    "def plot_results_bar(\n",
    "    df: pd.DataFrame,\n",
    "    metric_col: str,\n",
    "    metric_name: str,\n",
    "    method_names: List[str] = METHOD_NAMES,\n",
    "    figsize: Tuple[int, int] = (4, 3),\n",
    "    rot: int = 0,\n",
    "    plot_errs: bool = True,\n",
    "    label: bool = False,\n",
    "    **kwargs,\n",
    "):\n",
    "    grouped = df[metric_col].groupby(INDEX_COLS[0])\n",
    "    means = grouped.mean()\n",
    "    errs = grouped.std() if plot_errs else None\n",
    "    ax: Axes = means.loc[method_names].plot.bar(\n",
    "        yerr=errs,\n",
    "        capsize=4,\n",
    "        rot=rot,\n",
    "        xlabel=INDEX_COLS[0].capitalize(),\n",
    "        ylabel=metric_name,\n",
    "        figsize=figsize,\n",
    "        **kwargs,\n",
    "    )\n",
    "    ax.set_xlabel(\"\")\n",
    "\n",
    "    if label:\n",
    "        bars = ax.containers[-1]\n",
    "        ax.bar_label(\n",
    "            bars,\n",
    "            labels=[f\"{c.get_height():.2f}\" for c in bars],\n",
    "            label_type=\"edge\",\n",
    "            padding=5,\n",
    "            fontsize=10,\n",
    "        )\n",
    "\n",
    "\n",
    "def plot_metrics(\n",
    "    df: pd.DataFrame,\n",
    "    metrics: List[Metrics],\n",
    "    method_names: List[str] = METHOD_NAMES,\n",
    "    figsize: Tuple[int, int] = (4, 3),\n",
    "    rot: int = 0,\n",
    "    label: bool = False,\n",
    "):\n",
    "    for metric in metrics:\n",
    "        plot_results_bar(\n",
    "            df,\n",
    "            metric.col,\n",
    "            metric.name,\n",
    "            method_names=method_names,\n",
    "            rot=rot,\n",
    "            plot_errs=metric.plot_errors,\n",
    "            ylim=metric.ylim,\n",
    "            figsize=figsize,\n",
    "            label=label,\n",
    "        )\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def load_benchmark_results(\n",
    "    result_dir: Path,\n",
    "    benchmark_name_map: Dict[str, str],\n",
    "    benchmarks: Optional[List[str]] = None,\n",
    "    metric: Optional[Metrics] = None,\n",
    "    baseline: Optional[str] = None,\n",
    ") -> pd.DataFrame:\n",
    "    combined_df = pd.DataFrame()\n",
    "    if benchmarks is not None:\n",
    "        benchmark_name_map = {\n",
    "            k: v for k, v in benchmark_name_map.items() if k in benchmarks\n",
    "        }\n",
    "    for bench_id, bench_name in benchmark_name_map.items():\n",
    "        df = pd.read_csv(result_dir / f\"cleaned/{bench_id}.csv\")\n",
    "        df[INDEX_COLS[0]] = df[INDEX_COLS[0]].str.upper()\n",
    "        df[\"benchmark\"] = bench_name\n",
    "        df = df.set_index(INDEX_COLS + [\"benchmark\"])[METRIC_COLS].drop_duplicates()\n",
    "        if metric is not None:\n",
    "            assert baseline is not None\n",
    "            df[metric.col] = df[metric.col] / df[metric.col].loc[baseline].mean().item()\n",
    "        combined_df = pd.concat([combined_df, df])\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "def plot_normalized_metric(\n",
    "    result_dir: Path,\n",
    "    metric: Metrics,\n",
    "    methods: List[str],\n",
    "    baseline: str,\n",
    "    benchmark_name_map: Dict[str, str],\n",
    "    rotation: int = -15,\n",
    "    label_padding: float = 5,\n",
    "    benchmarks: Optional[List[str]] = None,\n",
    "    **kwargs,\n",
    "):\n",
    "    combined_df = load_benchmark_results(\n",
    "        result_dir, benchmark_name_map, benchmarks, metric, baseline\n",
    "    )\n",
    "\n",
    "    kwargs = (\n",
    "        dict(\n",
    "            height=4,\n",
    "            aspect=2,\n",
    "            palette=\"muted\",\n",
    "            capsize=0.2,\n",
    "            err_kws={\"linewidth\": 1.5},\n",
    "        )\n",
    "        | kwargs\n",
    "    )\n",
    "\n",
    "    g = sns.catplot(\n",
    "        data=combined_df.loc[methods],\n",
    "        x=\"benchmark\",\n",
    "        y=metric.col,\n",
    "        hue=\"method\",\n",
    "        kind=\"bar\",\n",
    "        errorbar=\"sd\" if metric.plot_errors else None,\n",
    "        legend_out=False,\n",
    "        **kwargs,\n",
    "    )\n",
    "    g.set_axis_labels(\"\", metric.name)\n",
    "    g.set_xticklabels(rotation=rotation)\n",
    "    # g.legend.set_title(\"\")\n",
    "    handles, labels = g.axes.item().get_legend_handles_labels()\n",
    "    g.legend.remove()\n",
    "    g.fig.legend(\n",
    "        handles,\n",
    "        labels,\n",
    "        ncol=len(methods),\n",
    "        loc=\"upper center\",\n",
    "        bbox_to_anchor=(0.5, 1.1),\n",
    "        frameon=False,\n",
    "    )\n",
    "\n",
    "    ax = g.facet_axis(0, 0)\n",
    "    for c in ax.containers:\n",
    "        labels = [f\"{v.get_height():.2f}\" for v in c]\n",
    "        ax.bar_label(\n",
    "            c, labels=labels, label_type=\"edge\", padding=label_padding, fontsize=8\n",
    "        )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_normalized_metrics(\n",
    "    result_dir: Path,\n",
    "    metrics: List[Metrics],\n",
    "    methods: List[str],\n",
    "    baselines: Union[str, List[str]],\n",
    "    benchmark_name_map: Dict[str, str],\n",
    "    **kwargs,\n",
    "):\n",
    "    if isinstance(baselines, str):\n",
    "        baselines = [baselines] * len(metrics)\n",
    "    for metric, baseline in zip(metrics, baselines):\n",
    "        plot_normalized_metric(\n",
    "            result_dir, metric, methods, baseline, benchmark_name_map, **kwargs\n",
    "        )\n",
    "\n",
    "\n",
    "def plot_gpu_cpu_cache_hit_rate_per_benchmark(\n",
    "    result_dir: Path,\n",
    "    benchmark_name_map: Dict[str, str],\n",
    "    benchmarks: Optional[List[str]] = None,\n",
    "    rotation: int = -15,\n",
    "    figsize: Tuple[float, float] = (7, 4),\n",
    "):\n",
    "    df = load_benchmark_results(result_dir, benchmark_name_map, benchmarks)\n",
    "    df = df.groupby([\"benchmark\", \"method\"]).mean().reset_index()\n",
    "\n",
    "    _, ax = plt.subplots(figsize=figsize)\n",
    "    bar_width = 0.35\n",
    "\n",
    "    benchmarks = df[\"benchmark\"].unique()\n",
    "    colors = sns.color_palette(\"muted\").as_hex()\n",
    "    for i, benchmark in enumerate(benchmark_name_map.values()):\n",
    "        # Filter data for each benchmark and method\n",
    "        apc_data = df[(df[\"benchmark\"] == benchmark) & (df[\"method\"] == \"APC\")]\n",
    "        mt_apc_data = df[(df[\"benchmark\"] == benchmark) & (df[\"method\"] == \"MT-APC\")]\n",
    "\n",
    "        # Plot stacked bars for APC and MT-APC\n",
    "        ax.bar(\n",
    "            i - bar_width / 2,\n",
    "            apc_data[\"gpu_prefix_cache_hit_rate\"],\n",
    "            bar_width,\n",
    "            label=\"APC GPU\" if i == 0 else \"\",\n",
    "            color=colors[0],\n",
    "        )\n",
    "        ax.bar(\n",
    "            i + bar_width / 2,\n",
    "            mt_apc_data[\"gpu_prefix_cache_hit_rate\"],\n",
    "            bar_width,\n",
    "            label=\"MT-APC GPU\" if i == 0 else \"\",\n",
    "            color=colors[1],\n",
    "        )\n",
    "        ax.bar(\n",
    "            i + bar_width / 2,\n",
    "            mt_apc_data[\"cpu_prefix_cache_hit_rate\"],\n",
    "            bar_width,\n",
    "            bottom=mt_apc_data[\"gpu_prefix_cache_hit_rate\"],\n",
    "            label=\"MT-APC CPU\" if i == 0 else \"\",\n",
    "            color=colors[2],\n",
    "        )\n",
    "\n",
    "    ax.set_xticks(range(len(benchmarks)))\n",
    "    ax.set_xticklabels(benchmark_name_map.values(), rotation=rotation)\n",
    "    # ax.set_xlabel(\"Workload\")\n",
    "    ax.set_ylabel(\"Prefix Cache Hit Rate (%)\")\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.legend(loc=\"upper right\", fontsize=9)\n",
    "    # ax.legend(ncols=3, loc=\"upper right\", fontsize=9)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_gpu_cpu_cache_hit_rate(\n",
    "    df: pd.DataFrame,\n",
    "    method_names: List[str] = METHOD_NAMES,\n",
    "    metric_name: str = \"Prefix Cache Hit Rate (%)\",\n",
    "    plot_errors: bool = False,\n",
    "    rotation: int = -15,\n",
    "    label_padding: float = 5,\n",
    "    **kwargs,\n",
    "):\n",
    "    kwargs = (\n",
    "        dict(\n",
    "            height=4,\n",
    "            aspect=2,\n",
    "            palette=\"muted\",\n",
    "            capsize=0.2,\n",
    "            err_kws={\"linewidth\": 1.5},\n",
    "        )\n",
    "        | kwargs\n",
    "    )\n",
    "    df = df.reset_index().melt(\n",
    "        id_vars=[\"method\", \"iteration\"],\n",
    "        value_vars=[\"gpu_prefix_cache_hit_rate\", \"cpu_prefix_cache_hit_rate\"],\n",
    "        var_name=\"device\",\n",
    "        value_name=\"hit_rate\",\n",
    "    )\n",
    "    df[\"device\"] = df[\"device\"].str.replace(\"_prefix_cache_hit_rate\", \"\").str.upper()\n",
    "    df = df.set_index([\"method\", \"iteration\"])\n",
    "\n",
    "    g = sns.catplot(\n",
    "        data=df.loc[method_names],\n",
    "        x=\"method\",\n",
    "        y=\"hit_rate\",\n",
    "        hue=\"device\",\n",
    "        kind=\"bar\",\n",
    "        errorbar=\"sd\" if plot_errors else None,\n",
    "        **kwargs,\n",
    "    )\n",
    "    g.set_axis_labels(\"Method\", metric_name)\n",
    "    g.set_xticklabels(rotation=rotation)\n",
    "    g.legend.set_title(\"\")\n",
    "\n",
    "    ax = g.facet_axis(0, 0)\n",
    "    for c in ax.containers:\n",
    "        labels = [f\"{v.get_height():.2f}\" for v in c]\n",
    "        ax.bar_label(\n",
    "            c, labels=labels, label_type=\"edge\", padding=label_padding, fontsize=10\n",
    "        )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Workloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BENCHMARK_NAME_MAP = {\n",
    "    \"sharegpt\": \"ShareGPT\",\n",
    "    \"generative_agents\": \"Generative Agents\",\n",
    "    \"multiturn_long\": \"Multi-turn Chat (Long)\",\n",
    "    \"multiturn_short\": \"Multi-turn Chat (Short)\",\n",
    "    \"guessing_game\": \"Guessing Game\",\n",
    "}\n",
    "\n",
    "RESULT_DIR = ROOT_RESULT_DIR / \"standard\"\n",
    "METHOD_NAMES.clear()\n",
    "METHOD_NAMES.extend(get_method_names(RESULT_DIR))\n",
    "METHOD_NAMES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ShareGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_name = \"sharegpt\"\n",
    "result_dir = RESULT_DIR / \"cleaned\"\n",
    "result_df = pd.read_csv(result_dir / f\"{bench_name}.csv\")\n",
    "metric_cols = METRIC_COLS\n",
    "result_df[INDEX_COLS[0]] = result_df[INDEX_COLS[0]].str.upper()\n",
    "result_df = result_df.set_index(INDEX_COLS)[metric_cols].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(result_df.groupby([INDEX_COLS[0]]).mean().loc[METHOD_NAMES])\n",
    "display(result_df.groupby([INDEX_COLS[0]]).std().loc[METHOD_NAMES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    Metrics(\n",
    "        col=\"duration\",\n",
    "        name=\"Job Completion Time (s)\",\n",
    "        ylim=(0, 60),\n",
    "    ),\n",
    "    Metrics(\n",
    "        col=\"request_throughput\",\n",
    "        name=\"Request Throughput (request/s)\",\n",
    "        ylim=(0, 3.5),\n",
    "    ),\n",
    "    Metrics(\n",
    "        col=\"output_throughput\",\n",
    "        name=\"Output Throughput (tps)\",\n",
    "        ylim=(0, 690),\n",
    "    ),\n",
    "    Metrics(\n",
    "        col=\"median_ttft_ms\",\n",
    "        name=\"Median TTFT (ms)\",\n",
    "        ylim=(0, 95),\n",
    "        plot_errors=False,\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(result_df, metrics=metrics, method_names=METHOD_NAMES[:3], figsize=(4, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(result_df, metrics=metrics, method_names=METHOD_NAMES[2:], figsize=(6, 3), rot=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Generative Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_name = \"generative_agents\"\n",
    "result_dir = RESULT_DIR / \"cleaned\"\n",
    "result_df = pd.read_csv(result_dir / f\"{bench_name}.csv\")\n",
    "metric_cols = METRIC_COLS\n",
    "result_df[INDEX_COLS[0]] = result_df[INDEX_COLS[0]].str.upper()\n",
    "result_df = result_df.set_index(INDEX_COLS)[metric_cols].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(result_df.groupby([INDEX_COLS[0]]).mean().loc[METHOD_NAMES])\n",
    "display(result_df.groupby([INDEX_COLS[0]]).std().loc[METHOD_NAMES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    Metrics(\n",
    "        col=\"duration\",\n",
    "        name=\"Job Completion Time (s)\",\n",
    "        ylim=(0, 25),\n",
    "    ),\n",
    "    Metrics(\n",
    "        col=\"request_throughput\",\n",
    "        name=\"Request Throughput (request/s)\",\n",
    "        ylim=(0, 10),\n",
    "    ),\n",
    "    Metrics(\n",
    "        col=\"output_throughput\",\n",
    "        name=\"Output Throughput (tps)\",\n",
    "        ylim=(0, 38),\n",
    "    ),\n",
    "    Metrics(\n",
    "        col=\"median_ttft_ms\",\n",
    "        name=\"Median TTFT (ms)\",\n",
    "        ylim=(0, 60),\n",
    "        plot_errors=False,\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(result_df, metrics=metrics, method_names=METHOD_NAMES[:3], figsize=(4, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(result_df, metrics=metrics, method_names=METHOD_NAMES[2:], figsize=(6, 3), rot=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiturn Chat (Long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_name = \"multiturn_long\"\n",
    "result_dir = RESULT_DIR / \"cleaned\"\n",
    "result_df = pd.read_csv(result_dir / f\"{bench_name}.csv\")\n",
    "metric_cols = METRIC_COLS\n",
    "result_df[INDEX_COLS[0]] = result_df[INDEX_COLS[0]].str.upper()\n",
    "result_df = result_df.set_index(INDEX_COLS)[metric_cols].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(result_df.groupby([INDEX_COLS[0]]).mean().loc[METHOD_NAMES])\n",
    "display(result_df.groupby([INDEX_COLS[0]]).std().loc[METHOD_NAMES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    Metrics(\n",
    "        col=\"duration\",\n",
    "        name=\"Job Completion Time (s)\",\n",
    "        ylim=(0, 21),\n",
    "    ),\n",
    "    Metrics(\n",
    "        col=\"request_throughput\",\n",
    "        name=\"Request Throughput (request/s)\",\n",
    "        ylim=(0, 2.2),\n",
    "    ),\n",
    "    Metrics(\n",
    "        col=\"output_throughput\",\n",
    "        name=\"Output Throughput (tps)\",\n",
    "        ylim=(0, 205),\n",
    "    ),\n",
    "    Metrics(\n",
    "        col=\"median_ttft_ms\",\n",
    "        name=\"Median TTFT (ms)\",\n",
    "        ylim=(0, 1100),\n",
    "        plot_errors=False,\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(result_df, metrics=metrics, method_names=METHOD_NAMES[:3], figsize=(4, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    Metrics(\n",
    "        col=\"duration\",\n",
    "        name=\"Job Completion Time (s)\",\n",
    "        ylim=(0, 21),\n",
    "    ),\n",
    "    Metrics(\n",
    "        col=\"request_throughput\",\n",
    "        name=\"Request Throughput (request/s)\",\n",
    "        ylim=(0, 2.2),\n",
    "    ),\n",
    "    Metrics(\n",
    "        col=\"output_throughput\",\n",
    "        name=\"Output Throughput (tps)\",\n",
    "        ylim=(0, 205),\n",
    "    ),\n",
    "    Metrics(\n",
    "        col=\"median_ttft_ms\",\n",
    "        name=\"Median TTFT (ms)\",\n",
    "        ylim=(0, 1250),\n",
    "        plot_errors=False,\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(result_df, metrics=metrics, method_names=METHOD_NAMES[2:], figsize=(6, 3), rot=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiturn Chat (Short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_name = \"multiturn_short\"\n",
    "result_dir = RESULT_DIR / \"cleaned\"\n",
    "result_df = pd.read_csv(result_dir / f\"{bench_name}.csv\")\n",
    "metric_cols = METRIC_COLS\n",
    "result_df[INDEX_COLS[0]] = result_df[INDEX_COLS[0]].str.upper()\n",
    "result_df = result_df.set_index(INDEX_COLS)[metric_cols].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(result_df.groupby([INDEX_COLS[0]]).mean().loc[METHOD_NAMES])\n",
    "display(result_df.groupby([INDEX_COLS[0]]).std().loc[METHOD_NAMES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    Metrics(\n",
    "        col=\"duration\",\n",
    "        name=\"Job Completion Time (s)\",\n",
    "        ylim=(0, 13),\n",
    "    ),\n",
    "    Metrics(\n",
    "        col=\"request_throughput\",\n",
    "        name=\"Request Throughput (request/s)\",\n",
    "        ylim=(0, 7),\n",
    "    ),\n",
    "    Metrics(\n",
    "        col=\"output_throughput\",\n",
    "        name=\"Output Throughput (tps)\",\n",
    "        ylim=(0, 42),\n",
    "    ),\n",
    "    Metrics(\n",
    "        col=\"median_ttft_ms\",\n",
    "        name=\"Median TTFT (ms)\",\n",
    "        ylim=(0, 1700),\n",
    "        plot_errors=False,\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(result_df, metrics=metrics, method_names=METHOD_NAMES[:3], figsize=(4, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(result_df, metrics=metrics, method_names=METHOD_NAMES[2:], figsize=(6, 3), rot=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guessing Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_name = \"guessing_game\"\n",
    "result_dir = RESULT_DIR / \"cleaned\"\n",
    "result_df = pd.read_csv(result_dir / f\"{bench_name}.csv\")\n",
    "metric_cols = METRIC_COLS\n",
    "result_df[INDEX_COLS[0]] = result_df[INDEX_COLS[0]].str.upper()\n",
    "result_df = result_df.set_index(INDEX_COLS)[metric_cols].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(result_df.groupby([INDEX_COLS[0]]).mean().loc[METHOD_NAMES])\n",
    "display(result_df.groupby([INDEX_COLS[0]]).std().loc[METHOD_NAMES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    Metrics(\n",
    "        col=\"duration\",\n",
    "        name=\"Job Completion Time (s)\",\n",
    "        ylim=(0, 8.5),\n",
    "    ),\n",
    "    Metrics(\n",
    "        col=\"request_throughput\",\n",
    "        name=\"Request Throughput (request/s)\",\n",
    "        ylim=(0, 60),\n",
    "    ),\n",
    "    Metrics(\n",
    "        col=\"output_throughput\",\n",
    "        name=\"Output Throughput (tps)\",\n",
    "        ylim=(0, 200),\n",
    "    ),\n",
    "    Metrics(\n",
    "        col=\"median_ttft_ms\",\n",
    "        name=\"Median TTFT (ms)\",\n",
    "        ylim=(0, 600),\n",
    "        plot_errors=False,\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(result_df, metrics=metrics, method_names=METHOD_NAMES[:3], figsize=(4, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(result_df, metrics=metrics, method_names=METHOD_NAMES[2:], figsize=(6, 3), rot=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache hit rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.DataFrame()\n",
    "for bench_id, bench_name in BENCHMARK_NAME_MAP.items():\n",
    "    df = pd.read_csv(RESULT_DIR / f\"cleaned/{bench_id}.csv\")\n",
    "    df = df[df[\"method\"] == \"apc\"]\n",
    "    df = df[[\"method\", \"gpu_prefix_cache_hit_rate\"]]\n",
    "    df = df.groupby(\"method\").mean()\n",
    "    df[\"benchmark\"] = bench_name\n",
    "    combined_df = pd.concat([combined_df, df])\n",
    "\n",
    "g = sns.catplot(\n",
    "    data=combined_df,\n",
    "    x=\"benchmark\",\n",
    "    y=\"gpu_prefix_cache_hit_rate\",\n",
    "    kind=\"bar\",\n",
    "    errorbar=None,\n",
    "    height=2.75,\n",
    "    aspect=2,\n",
    ")\n",
    "g.set_axis_labels(\"\", \"Prefix Cache Hit Rate (%)\")\n",
    "g.set_xticklabels(rotation=-15)\n",
    "g.set(ylim=(0, 100))\n",
    "\n",
    "ax = g.facet_axis(0, 0)\n",
    "for c in ax.containers:\n",
    "    labels = [f\"{v.get_height():.2f}\" for v in c]\n",
    "    ax.bar_label(c, labels=labels, label_type=\"edge\", padding=5, fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    Metrics(\n",
    "        col=\"duration\",\n",
    "        name=\"JCT (normalized)\",\n",
    "    ),\n",
    "    Metrics(\n",
    "        col=\"request_throughput\",\n",
    "        name=\"Request Throughput\\n(normalized)\",\n",
    "    ),\n",
    "    Metrics(\n",
    "        col=\"output_throughput\",\n",
    "        name=\"Output Throughput\\n(normalized)\",\n",
    "    ),\n",
    "    Metrics(\n",
    "        col=\"median_ttft_ms\",\n",
    "        name=\"Median TTFT\\n(normalized)\",\n",
    "        plot_errors=False,\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_normalized_metrics(\n",
    "    RESULT_DIR,\n",
    "    metrics,\n",
    "    methods=METHOD_NAMES[:3],\n",
    "    baselines=[\"NO-APC\", \"MT-APC\", \"MT-APC\", \"NO-APC\"],\n",
    "    benchmark_name_map=BENCHMARK_NAME_MAP,\n",
    "    height=2.5,\n",
    "    aspect=2.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gpu_cpu_cache_hit_rate_per_benchmark(RESULT_DIR, BENCHMARK_NAME_MAP, figsize=(6, 2.75))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_name = \"guessing_game\"\n",
    "result_df = pd.read_csv(RESULT_DIR / f\"cleaned/{bench_name}.csv\")\n",
    "metric_cols = METRIC_COLS\n",
    "result_df[INDEX_COLS[0]] = result_df[INDEX_COLS[0]].str.upper()\n",
    "result_df = result_df.set_index(INDEX_COLS)[metric_cols].drop_duplicates()\n",
    "\n",
    "for col in metric_cols:\n",
    "    if col not in [\"gpu_prefix_cache_hit_rate\", \"cpu_prefix_cache_hit_rate\"]:\n",
    "        result_df[col] = result_df[col] / result_df[col].loc[\"MT-APC-ONLY-MT\"].mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(result_df.groupby([INDEX_COLS[0]]).mean().loc[METHOD_NAMES])\n",
    "display(result_df.groupby([INDEX_COLS[0]]).std().loc[METHOD_NAMES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    Metrics(\n",
    "        col=\"duration\",\n",
    "        name=\"JCT (normalized)\",\n",
    "        ylim=(0, 1.3),\n",
    "    ),\n",
    "    Metrics(\n",
    "        col=\"request_throughput\",\n",
    "        name=\"Request Throughput\\n(normalized)\",\n",
    "        ylim=(0, 1.3),\n",
    "    ),\n",
    "    Metrics(\n",
    "        col=\"output_throughput\",\n",
    "        name=\"Output Throughput\\n(normalized)\",\n",
    "        ylim=(0, 1.3),\n",
    "    ),\n",
    "    Metrics(\n",
    "        col=\"mean_ttft_ms\",\n",
    "        name=\"Mean TTFT\\n(normalized)\",\n",
    "        ylim=(0, 1.3),\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = result_df.copy()\n",
    "new_methods = [i[0] for i in tmp_df.index.to_list()]\n",
    "new_methods = [\n",
    "    method + \"-ALL\" if method == \"MT-APC\" else method for method in new_methods\n",
    "]\n",
    "new_methods = [method[len(\"MT-APC-\") :] for method in new_methods]\n",
    "tmp_df[\"method2\"] = new_methods\n",
    "tmp_df = (\n",
    "    tmp_df.reset_index()\n",
    "    .drop(columns=\"method\")\n",
    "    .rename(columns={\"method2\": \"method\"})\n",
    "    .set_index(INDEX_COLS)\n",
    ")\n",
    "method_names = [name[len(\"MT-APC-\") :] for name in METHOD_NAMES[3:-1]] + [\"ALL\"]\n",
    "\n",
    "plot_metrics(\n",
    "    tmp_df,\n",
    "    metrics=metrics,\n",
    "    method_names=method_names,\n",
    "    figsize=(5.5, 2.44),\n",
    "    rot=-15,\n",
    "    label=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gpu_cpu_cache_hit_rate(\n",
    "    result_df,\n",
    "    method_names=[METHOD_NAMES[1]] + METHOD_NAMES[3:] + [METHOD_NAMES[2]],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large-scale Multi-Agent Workloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BENCHMARK_NAME_MAP = {\n",
    "    \"gptswarm_mmlu\": \"GPTSwarm-MMLU\",\n",
    "    \"guessing_game_e2e\": \"Guessing Game (DP)\",\n",
    "    \"guessing_game_e2e_cot\": \"Guessing Game (CoT)\",\n",
    "}\n",
    "\n",
    "RESULT_DIR = ROOT_RESULT_DIR / \"multi_agent\"\n",
    "METHOD_NAMES.clear()\n",
    "METHOD_NAMES.extend(get_method_names(RESULT_DIR))\n",
    "METHOD_NAMES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPTSwarm-MMLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_name = \"gptswarm_mmlu\"\n",
    "result_dir = RESULT_DIR / \"cleaned\"\n",
    "result_df = pd.read_csv(result_dir / f\"{bench_name}.csv\")\n",
    "metric_cols = METRIC_COLS\n",
    "result_df[INDEX_COLS[0]] = result_df[INDEX_COLS[0]].str.upper()\n",
    "result_df = result_df.set_index(INDEX_COLS)[metric_cols].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(result_df.groupby([INDEX_COLS[0]]).mean().loc[METHOD_NAMES])\n",
    "display(result_df.groupby([INDEX_COLS[0]]).std().loc[METHOD_NAMES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    Metrics(\n",
    "        col=\"duration\",\n",
    "        name=\"Job Completion Time (s)\",\n",
    "        ylim=(0, 650),\n",
    "    ),\n",
    "    Metrics(\n",
    "        col=\"request_throughput\",\n",
    "        name=\"Request Throughput (request/s)\",\n",
    "        ylim=(0, 180),\n",
    "    ),\n",
    "    Metrics(\n",
    "        col=\"output_throughput\",\n",
    "        name=\"Output Throughput (tps)\",\n",
    "        ylim=(0, 300),\n",
    "    ),\n",
    "    Metrics(\n",
    "        col=\"median_ttft_ms\",\n",
    "        name=\"Median TTFT (ms)\",\n",
    "        ylim=(0, 16000),\n",
    "        plot_errors=False,\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(result_df, metrics=metrics, method_names=METHOD_NAMES[:3], figsize=(4, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(result_df, metrics=metrics, method_names=METHOD_NAMES[2:], figsize=(6, 3), rot=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guessing Game (Direct Prompting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_name = \"guessing_game_e2e\"\n",
    "result_dir = RESULT_DIR / \"cleaned\"\n",
    "result_df = pd.read_csv(result_dir / f\"{bench_name}.csv\")\n",
    "metric_cols = METRIC_COLS\n",
    "result_df[INDEX_COLS[0]] = result_df[INDEX_COLS[0]].str.upper()\n",
    "result_df = result_df.set_index(INDEX_COLS)[metric_cols].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(result_df.groupby([INDEX_COLS[0]]).mean().loc[METHOD_NAMES])\n",
    "display(result_df.groupby([INDEX_COLS[0]]).std().loc[METHOD_NAMES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    Metrics(\n",
    "        col=\"duration\",\n",
    "        name=\"Job Completion Time (s)\",\n",
    "        ylim=(0, 300),\n",
    "    ),\n",
    "    Metrics(\n",
    "        col=\"request_throughput\",\n",
    "        name=\"Request Throughput (request/s)\",\n",
    "        ylim=(0, 42),\n",
    "    ),\n",
    "    Metrics(\n",
    "        col=\"output_throughput\",\n",
    "        name=\"Output Throughput (tps)\",\n",
    "        ylim=(0, 150),\n",
    "    ),\n",
    "    Metrics(\n",
    "        col=\"median_ttft_ms\",\n",
    "        name=\"Median TTFT (ms)\",\n",
    "        ylim=(0, 23000),\n",
    "        plot_errors=False,\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(result_df, metrics=metrics, method_names=METHOD_NAMES[:3], figsize=(4, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(result_df, metrics=metrics, method_names=METHOD_NAMES[2:], figsize=(6, 3), rot=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guessing Game (CoT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_name = \"guessing_game_e2e_cot\"\n",
    "result_dir = RESULT_DIR / \"cleaned\"\n",
    "result_df = pd.read_csv(result_dir / f\"{bench_name}.csv\")\n",
    "metric_cols = METRIC_COLS\n",
    "result_df[INDEX_COLS[0]] = result_df[INDEX_COLS[0]].str.upper()\n",
    "result_df = result_df.set_index(INDEX_COLS)[metric_cols].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(result_df.groupby([INDEX_COLS[0]]).mean().loc[METHOD_NAMES])\n",
    "display(result_df.groupby([INDEX_COLS[0]]).std().loc[METHOD_NAMES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    Metrics(\n",
    "        col=\"duration\",\n",
    "        name=\"Job Completion Time (s)\",\n",
    "        ylim=(0, 1250),\n",
    "    ),\n",
    "    Metrics(\n",
    "        col=\"request_throughput\",\n",
    "        name=\"Request Throughput (request/s)\",\n",
    "        ylim=(0, 7),\n",
    "    ),\n",
    "    Metrics(\n",
    "        col=\"output_throughput\",\n",
    "        name=\"Output Throughput (tps)\",\n",
    "        ylim=(0, 950),\n",
    "    ),\n",
    "    Metrics(\n",
    "        col=\"median_ttft_ms\",\n",
    "        name=\"Median TTFT (ms)\",\n",
    "        ylim=(0, 68000),\n",
    "        plot_errors=False,\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(result_df, metrics=metrics, method_names=METHOD_NAMES[:3], figsize=(4, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(result_df, metrics=metrics, method_names=METHOD_NAMES[2:], figsize=(6, 3), rot=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache hit rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.DataFrame()\n",
    "for bench_id, bench_name in BENCHMARK_NAME_MAP.items():\n",
    "    df = pd.read_csv(RESULT_DIR / f\"cleaned/{bench_id}.csv\")\n",
    "    df = df[df[\"method\"] == \"apc\"]\n",
    "    df = df[[\"method\", \"gpu_prefix_cache_hit_rate\"]]\n",
    "    df = df.groupby(\"method\").mean()\n",
    "    df[\"benchmark\"] = bench_name\n",
    "    combined_df = pd.concat([combined_df, df])\n",
    "\n",
    "g = sns.catplot(\n",
    "    data=combined_df,\n",
    "    x=\"benchmark\",\n",
    "    y=\"gpu_prefix_cache_hit_rate\",\n",
    "    kind=\"bar\",\n",
    "    errorbar=None,\n",
    "    height=2.75,\n",
    "    aspect=2,\n",
    ")\n",
    "g.set_axis_labels(\"\", \"Prefix Cache Hit Rate (%)\")\n",
    "g.set_xticklabels(rotation=-15)\n",
    "g.set(ylim=(0, 100))\n",
    "\n",
    "ax = g.facet_axis(0, 0)\n",
    "for c in ax.containers:\n",
    "    labels = [f\"{v.get_height():.2f}\" for v in c]\n",
    "    ax.bar_label(c, labels=labels, label_type=\"edge\", padding=5, fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    Metrics(\n",
    "        col=\"duration\",\n",
    "        name=\"JCT (normalized)\",\n",
    "    ),\n",
    "    Metrics(\n",
    "        col=\"request_throughput\",\n",
    "        name=\"Request Throughput\\n(normalized)\",\n",
    "    ),\n",
    "    Metrics(\n",
    "        col=\"output_throughput\",\n",
    "        name=\"Output Throughput\\n(normalized)\",\n",
    "    ),\n",
    "    Metrics(\n",
    "        col=\"median_ttft_ms\",\n",
    "        name=\"Median TTFT\\n(normalized)\",\n",
    "        plot_errors=False,\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_normalized_metrics(\n",
    "    RESULT_DIR,\n",
    "    metrics,\n",
    "    methods=METHOD_NAMES[:3],\n",
    "    baselines=[\"NO-APC\", \"MT-APC\", \"MT-APC\", \"NO-APC\"],\n",
    "    benchmark_name_map=BENCHMARK_NAME_MAP,\n",
    "    rotation=0,\n",
    "    height=2.5,\n",
    "    aspect=2.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gpu_cpu_cache_hit_rate_per_benchmark(\n",
    "    RESULT_DIR, BENCHMARK_NAME_MAP, rotation=-15, figsize=(6, 2.75)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPTSwarm Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"SERVER_CONFIG_PATH\"] = str((ROOT_DIR / \".env\").absolute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from swarm.environment.operations.final_decision import MergingStrategy\n",
    "from swarm.graph.swarm import Swarm\n",
    "from swarm.llm import OPENAI_MODEL_PREFIX\n",
    "from swarm.utils.const import GPTSWARM_ROOT\n",
    "\n",
    "from agents.config import BaseClientConfig\n",
    "from agents.gptswarm.guessing_game.guessing_game import GuessTwoThirdGame\n",
    "\n",
    "\n",
    "config = BaseClientConfig()\n",
    "MODEL_NAME = OPENAI_MODEL_PREFIX + config.model\n",
    "\n",
    "\n",
    "def display_swarm_graph(swarm: Swarm, file_path: Path):\n",
    "    graph, _ = swarm.connection_dist.realize(swarm.composite_graph)\n",
    "    file_name = file_path.name\n",
    "    graph.display(file_name=file_name)\n",
    "    (GPTSWARM_ROOT / \"result\" / file_name).rename(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPTSwarm-MMLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swarm = Swarm(\n",
    "    agent_names=[\"IO\"] * 3 + [\"AdversarialAgent\"] * 3,\n",
    "    domain=\"mmlu\",\n",
    "    model_name=MODEL_NAME,\n",
    "    final_node_class=\"FinalDecision\",\n",
    "    final_node_kwargs=dict(strategy=MergingStrategy.MajorityVote),\n",
    "    edge_optimize=False,\n",
    ")\n",
    "display_swarm_graph(swarm, ROOT_RESULT_DIR / \"gptswarm_mmlu.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guessing Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = GuessTwoThirdGame(\n",
    "    model_name=OPENAI_MODEL_PREFIX + config.model,\n",
    "    num_participants=3,\n",
    "    num_steps=5,\n",
    ")\n",
    "display_swarm_graph(game, ROOT_RESULT_DIR / \"guessing_game.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elmas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
